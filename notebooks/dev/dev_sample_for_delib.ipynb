{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\boxplot.py:4: MatplotlibDeprecationWarning: \n",
      "The verbose.level rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "  from matplotlib.artist import setp\n"
     ]
    }
   ],
   "source": [
    "# My Standard Spark Session!\n",
    "\n",
    "# Python libraries:\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from dateutil import parser\n",
    "# import datetime\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import builtins\n",
    "import json\n",
    "import functools\n",
    "import operator\n",
    "from itertools import product\n",
    "\n",
    "# Numpy & Pandas!\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# Spark!\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"myapp\").getOrCreate()\n",
    "\n",
    "#     spark = SparkSession.builder.master(\"yarn\")\\\n",
    "#     .config(\"spark.executor.instances\", \"32\")\\\n",
    "#     .config(\"spark.executor.cores\", \"4\")\\\n",
    "#     .config(\"spark.executor.memory\", \"4G\")\\\n",
    "#     .config(\"spark.driver.memory\", \"4G\")\\\n",
    "#     .config(\"spark.executor.memoryOverhead\",\"4G\")\\\n",
    "#     .config(\"spark.yarn.queue\",\"Medium\")\\\n",
    "#     .appName(\"myapp\")\\\n",
    "#     .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "spark.conf.set(\"spark.sql.sources.partitionColumnTypeInference.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.debug.maxToStringFields\",\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Python Version:  3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]\n",
      "    Python Path:  C:\\ProgramData\\Anaconda3\n",
      " My Python Libs:  C:\\Users\\merz.d/.myconfigs\n",
      "   My Spark Dir:  C:\\Users\\merz.d/git/spark2_dfanalysis\n",
      "   My Spark Ctx:  <pyspark.sql.session.SparkSession object at 0x000001E3B4919B08>\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# The autoreload extension is already loaded. To reload it, use:\n",
    "#  %reload_ext autoreload\n",
    "\n",
    "\n",
    "# mylib:\n",
    "my_library = os.path.expanduser('~/.myconfigs')\n",
    "my_spark = os.path.expanduser('~/git/spark2_dfanalysis')\n",
    "sys.path.append(my_library)\n",
    "sys.path.append(my_spark)\n",
    "\n",
    "\n",
    "from shared.app_context import *\n",
    "from builder.DataFrameBuild import *\n",
    "\n",
    "ctx = ApplicationContext(\"Dev-Job\")\n",
    "\n",
    "DFB = DataFrameBuild(ctx.spark)\n",
    "\n",
    "print(\"%16s  %s\" % (\"Python Version:\",sys.version))\n",
    "print(\"%16s  %s\" % (\"Python Path:\",os.path.dirname(sys.executable)))\n",
    "print(\"%16s  %s\" % (\"My Python Libs:\",my_library))\n",
    "print(\"%16s  %s\" % (\"My Spark Dir:\",my_spark))\n",
    "print(\"%16s  %s\" % (\"My Spark Ctx:\",ctx.spark))\n",
    "# print(ctx.spark)\n",
    "# print(os.listdir(my_spark))\n",
    "# print(sys.path)\n",
    "# print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>name</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>age</th>\n",
       "      <th>net_worth</th>\n",
       "      <th>passphrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>katie</td>\n",
       "      <td>October</td>\n",
       "      <td>1993</td>\n",
       "      <td>27</td>\n",
       "      <td>259,057.80</td>\n",
       "      <td>14 sneaky black pigs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ron</td>\n",
       "      <td>June</td>\n",
       "      <td>1976</td>\n",
       "      <td>44</td>\n",
       "      <td>436,880.62</td>\n",
       "      <td>some 25 sly brown tigers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>chris</td>\n",
       "      <td>September</td>\n",
       "      <td>1982</td>\n",
       "      <td>38</td>\n",
       "      <td>556,448.20</td>\n",
       "      <td>$36.33 for the fast yellow mice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>connor</td>\n",
       "      <td>March</td>\n",
       "      <td>1987</td>\n",
       "      <td>33</td>\n",
       "      <td>543,575.91</td>\n",
       "      <td>a wonderful set of slippery purple kangaroos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>ray</td>\n",
       "      <td>February</td>\n",
       "      <td>1990</td>\n",
       "      <td>30</td>\n",
       "      <td>107,272.08</td>\n",
       "      <td>$36.33 for the slippery pink giraffes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>donnie</td>\n",
       "      <td>November</td>\n",
       "      <td>1962</td>\n",
       "      <td>58</td>\n",
       "      <td>1,230,047.52</td>\n",
       "      <td>2 sneaky gray giraffes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>katie</td>\n",
       "      <td>December</td>\n",
       "      <td>2007</td>\n",
       "      <td>13</td>\n",
       "      <td>58,025.30</td>\n",
       "      <td>nearly $15 worth of the large pink horses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>ellen</td>\n",
       "      <td>April</td>\n",
       "      <td>1998</td>\n",
       "      <td>22</td>\n",
       "      <td>60,927.94</td>\n",
       "      <td>only 17% of the large black dragons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>donnie</td>\n",
       "      <td>February</td>\n",
       "      <td>1966</td>\n",
       "      <td>54</td>\n",
       "      <td>474,514.57</td>\n",
       "      <td>5 large green kangaroos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>connor</td>\n",
       "      <td>May</td>\n",
       "      <td>2000</td>\n",
       "      <td>20</td>\n",
       "      <td>199,225.88</td>\n",
       "      <td>9.77 slow red cats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>erica</td>\n",
       "      <td>June</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>1,966.95</td>\n",
       "      <td>9.77 burning gray horses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>chris</td>\n",
       "      <td>May</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>3,109.84</td>\n",
       "      <td>5 quick yellow tigers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>melinda</td>\n",
       "      <td>September</td>\n",
       "      <td>1991</td>\n",
       "      <td>29</td>\n",
       "      <td>226,174.69</td>\n",
       "      <td>nearly $15 worth of the large red snakes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>josh</td>\n",
       "      <td>July</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>211.73</td>\n",
       "      <td>41 quick blue wolves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>ryan</td>\n",
       "      <td>April</td>\n",
       "      <td>1959</td>\n",
       "      <td>61</td>\n",
       "      <td>1,234,738.59</td>\n",
       "      <td>9.77 fast blue dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>ray</td>\n",
       "      <td>July</td>\n",
       "      <td>1952</td>\n",
       "      <td>68</td>\n",
       "      <td>574,236.49</td>\n",
       "      <td>5 quick gray foxes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>ron</td>\n",
       "      <td>August</td>\n",
       "      <td>1972</td>\n",
       "      <td>48</td>\n",
       "      <td>595,007.59</td>\n",
       "      <td>almost half of the sneaky blue foxes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>ellen</td>\n",
       "      <td>November</td>\n",
       "      <td>1976</td>\n",
       "      <td>44</td>\n",
       "      <td>323,933.98</td>\n",
       "      <td>5 sneaky gray dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>doug</td>\n",
       "      <td>January</td>\n",
       "      <td>1955</td>\n",
       "      <td>65</td>\n",
       "      <td>135,667.24</td>\n",
       "      <td>14 large black elephants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>ryan</td>\n",
       "      <td>December</td>\n",
       "      <td>2004</td>\n",
       "      <td>16</td>\n",
       "      <td>1,461.66</td>\n",
       "      <td>only 17% of the sneaky pink dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>gary</td>\n",
       "      <td>February</td>\n",
       "      <td>1961</td>\n",
       "      <td>59</td>\n",
       "      <td>1,242,666.39</td>\n",
       "      <td>almost half of the ugly green squirrels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>melinda</td>\n",
       "      <td>July</td>\n",
       "      <td>1997</td>\n",
       "      <td>23</td>\n",
       "      <td>11,105.46</td>\n",
       "      <td>5 burning gray whales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>katie</td>\n",
       "      <td>October</td>\n",
       "      <td>1974</td>\n",
       "      <td>46</td>\n",
       "      <td>893,362.37</td>\n",
       "      <td>almost half of the portly black snakes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>steve</td>\n",
       "      <td>April</td>\n",
       "      <td>1963</td>\n",
       "      <td>57</td>\n",
       "      <td>407,208.28</td>\n",
       "      <td>9.77 sneaky blue rabbits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>ethan</td>\n",
       "      <td>March</td>\n",
       "      <td>1951</td>\n",
       "      <td>69</td>\n",
       "      <td>841,193.34</td>\n",
       "      <td>some 25 beautiful blue tigers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index     name      month  year  age            net_worth  \\\n",
       "0       0    katie    October  1993   27           259,057.80   \n",
       "1       1      ron       June  1976   44           436,880.62   \n",
       "2       2    chris  September  1982   38           556,448.20   \n",
       "3       3   connor      March  1987   33           543,575.91   \n",
       "4       4      ray   February  1990   30           107,272.08   \n",
       "5       5   donnie   November  1962   58         1,230,047.52   \n",
       "6       6    katie   December  2007   13            58,025.30   \n",
       "7       7    ellen      April  1998   22            60,927.94   \n",
       "8       8   donnie   February  1966   54           474,514.57   \n",
       "9       9   connor        May  2000   20           199,225.88   \n",
       "10     10    erica       June  2017    3             1,966.95   \n",
       "11     11    chris        May  2017    3             3,109.84   \n",
       "12     12  melinda  September  1991   29           226,174.69   \n",
       "13     13     josh       July  2019    1               211.73   \n",
       "14     14     ryan      April  1959   61         1,234,738.59   \n",
       "15     15      ray       July  1952   68           574,236.49   \n",
       "16     16      ron     August  1972   48           595,007.59   \n",
       "17     17    ellen   November  1976   44           323,933.98   \n",
       "18     18     doug    January  1955   65           135,667.24   \n",
       "19     19     ryan   December  2004   16             1,461.66   \n",
       "20     20     gary   February  1961   59         1,242,666.39   \n",
       "21     21  melinda       July  1997   23            11,105.46   \n",
       "22     22    katie    October  1974   46           893,362.37   \n",
       "23     23    steve      April  1963   57           407,208.28   \n",
       "24     24    ethan      March  1951   69           841,193.34   \n",
       "\n",
       "                                      passphrase  \n",
       "0                           14 sneaky black pigs  \n",
       "1                       some 25 sly brown tigers  \n",
       "2                $36.33 for the fast yellow mice  \n",
       "3   a wonderful set of slippery purple kangaroos  \n",
       "4          $36.33 for the slippery pink giraffes  \n",
       "5                         2 sneaky gray giraffes  \n",
       "6      nearly $15 worth of the large pink horses  \n",
       "7            only 17% of the large black dragons  \n",
       "8                        5 large green kangaroos  \n",
       "9                             9.77 slow red cats  \n",
       "10                      9.77 burning gray horses  \n",
       "11                         5 quick yellow tigers  \n",
       "12      nearly $15 worth of the large red snakes  \n",
       "13                          41 quick blue wolves  \n",
       "14                           9.77 fast blue dogs  \n",
       "15                            5 quick gray foxes  \n",
       "16          almost half of the sneaky blue foxes  \n",
       "17                            5 sneaky gray dogs  \n",
       "18                      14 large black elephants  \n",
       "19              only 17% of the sneaky pink dogs  \n",
       "20       almost half of the ugly green squirrels  \n",
       "21                         5 burning gray whales  \n",
       "22        almost half of the portly black snakes  \n",
       "23                      9.77 sneaky blue rabbits  \n",
       "24                 some 25 beautiful blue tigers  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6 columns: strings, ints, float\n",
    "import random\n",
    "\n",
    "num = 500\n",
    "\n",
    "\n",
    "# Lists / arrays\n",
    "lst_months = ['January','February','March','April','May','June',\n",
    "              'July','August','September','October','November','December']\n",
    "\n",
    "indices = [int(x) for x in np.linspace(0,num-1,num)]\n",
    "# print(indices)\n",
    "\n",
    "years = [random.randint(1950,2020) for x in range(num)]\n",
    "# print(years)\n",
    "\n",
    "ages = np.array([(2020 - x) for x in years])\n",
    "multipliers = ages * ages * 0.5\n",
    "# print(ages)\n",
    "\n",
    "months = [random.choice(lst_months) for x in range(num)]\n",
    "# print(months)\n",
    "\n",
    "worth1 = np.array([random.random() * 1000 for x in range(num)])\n",
    "worth = worth1 * multipliers\n",
    "print(len(worth))\n",
    "\n",
    "lst_names = ['sarah','hannah','jessica','ellen','bill','steve','mary',\n",
    "             'alyssa','brian','elizabeth','josh','ryan','katie','connor',\n",
    "             'erica','lisa','doug','stacie','chris','gary','tom','ellen',\n",
    "             'paula','samuel','ethan','dan','peter','randall', 'courtney',\n",
    "             'ron', 'andrew', 'ray', 'donnie', 'jeremy', 'isabel', 'melinda']\n",
    "\n",
    "\n",
    "names = [random.choice(lst_names) for x in range(num)]\n",
    "\n",
    "lst_adj = ['quick', 'fast', 'slow', 'slippery', 'sly', 'sneaky', 'large', 'small', 'portly', 'burning', 'wet', 'dirty',\n",
    "           'beautiful', 'ugly']\n",
    "lst_color = ['red', 'orange', 'yellow', 'green', 'blue', 'purple', 'pink', 'cyan', 'gray', 'black', 'brown']\n",
    "lst_animal = ['kangaroos', 'dragons', 'rabbits', 'frogs', 'foxes', 'whales', 'dogs', 'cats', 'tigers', 'snakes', \n",
    "              'elephants', 'squirrels',\n",
    "              'pigs', 'horses', 'mice', 'giraffes', 'wolves']\n",
    "\n",
    "lst_catch = ['nearly $15 worth of the', 'only 17% of the', 'some 25', '14', '12.5% of the', '9.77', '$36.33 for the', '41', '$50 worth of the', '2',\n",
    "             '10% of the', 'almost half of the', 'a wonderful set of', '5']\n",
    "\n",
    "\n",
    "phrases = [\"{} {} {} {}\".format(random.choice(lst_catch),\n",
    "                                random.choice(lst_adj),\n",
    "                             random.choice(lst_color),\n",
    "                             random.choice(lst_animal))\n",
    "                             for x in range(num)]\n",
    "\n",
    "\n",
    "# Create Array!\n",
    "df = DFB.arrays_to_dataframe([indices,names,months,years,ages,worth, phrases],\n",
    "                             ['index','name','month','year','age','net_worth', 'passphrase'])\n",
    "\n",
    "\n",
    "df.limit(25).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg = df.groupBy(\"age\")\\\n",
    ".agg(mean(\"net_worth\"))\\\n",
    ".orderBy(desc(\"age\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>avg(net_worth)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>1,450,157.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>1,047,076.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>862,107.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>915,027.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>66</td>\n",
       "      <td>982,774.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>65</td>\n",
       "      <td>1,067,480.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>64</td>\n",
       "      <td>830,836.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>63</td>\n",
       "      <td>1,182,579.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>62</td>\n",
       "      <td>1,050,842.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>61</td>\n",
       "      <td>1,107,449.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>655,409.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>59</td>\n",
       "      <td>1,155,146.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>58</td>\n",
       "      <td>944,626.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>57</td>\n",
       "      <td>609,272.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>56</td>\n",
       "      <td>846,286.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>55</td>\n",
       "      <td>458,458.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>54</td>\n",
       "      <td>708,081.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>53</td>\n",
       "      <td>718,600.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>52</td>\n",
       "      <td>364,410.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>51</td>\n",
       "      <td>617,256.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>592,237.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>49</td>\n",
       "      <td>867,968.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>48</td>\n",
       "      <td>472,504.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>47</td>\n",
       "      <td>558,236.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>46</td>\n",
       "      <td>473,778.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age       avg(net_worth)\n",
       "0    70         1,450,157.87\n",
       "1    69         1,047,076.19\n",
       "2    68           862,107.33\n",
       "3    67           915,027.91\n",
       "4    66           982,774.50\n",
       "5    65         1,067,480.05\n",
       "6    64           830,836.73\n",
       "7    63         1,182,579.66\n",
       "8    62         1,050,842.34\n",
       "9    61         1,107,449.22\n",
       "10   60           655,409.92\n",
       "11   59         1,155,146.74\n",
       "12   58           944,626.05\n",
       "13   57           609,272.42\n",
       "14   56           846,286.09\n",
       "15   55           458,458.13\n",
       "16   54           708,081.83\n",
       "17   53           718,600.19\n",
       "18   52           364,410.25\n",
       "19   51           617,256.16\n",
       "20   50           592,237.53\n",
       "21   49           867,968.26\n",
       "22   48           472,504.80\n",
       "23   47           558,236.09\n",
       "24   46           473,778.24"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_avg.limit(25).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\merz.d/git/delib/examples/spark_utils/data/\n",
      "True\n",
      "C:\\Users\\merz.d/git/delib/examples/spark_utils/data/basic_people_year_worth.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "my_dir = os.path.expanduser('~/git/delib/examples/spark_utils/data/')\n",
    "print(my_dir)\n",
    "\n",
    "print(os.path.isdir(my_dir))\n",
    "\n",
    "pfile = os.path.join(my_dir, \"basic_people_year_worth.snappy.parquet\")\n",
    "print(pfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o656.parquet.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 35.0 failed 1 times, most recent failure: Lost task 5.0 in stage 35.0 (TID 895, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\merz.d\\git\\delib\\examples\\spark_utils\\data\\basic_people_year_worth.snappy.parquet\\_temporary\\0\\_temporary\\attempt_20200414231155_0035_m_000005_895\\part-00005-3aeeca46-9d6e-42af-aadb-88242c5ac047-c000.snappy.parquet\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:120)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:236)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\r\n\t... 33 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\merz.d\\git\\delib\\examples\\spark_utils\\data\\basic_people_year_worth.snappy.parquet\\_temporary\\0\\_temporary\\attempt_20200414231155_0035_m_000005_895\\part-00005-3aeeca46-9d6e-42af-aadb-88242c5ac047-c000.snappy.parquet\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:120)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:236)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-7bbc59640ba0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"overwrite\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 844\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o656.parquet.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 35.0 failed 1 times, most recent failure: Lost task 5.0 in stage 35.0 (TID 895, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\merz.d\\git\\delib\\examples\\spark_utils\\data\\basic_people_year_worth.snappy.parquet\\_temporary\\0\\_temporary\\attempt_20200414231155_0035_m_000005_895\\part-00005-3aeeca46-9d6e-42af-aadb-88242c5ac047-c000.snappy.parquet\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:120)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:236)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\r\n\t... 33 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\merz.d\\git\\delib\\examples\\spark_utils\\data\\basic_people_year_worth.snappy.parquet\\_temporary\\0\\_temporary\\attempt_20200414231155_0035_m_000005_895\\part-00005-3aeeca46-9d6e-42af-aadb-88242c5ac047-c000.snappy.parquet\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:120)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:236)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").parquet(pfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImmediateDeprecationError",
     "evalue": "\nGoogle finance has been immediately deprecated due to large breaks in the API without the\nintroduction of a stable replacement. Pull Requests to re-enable these data\nconnectors are welcome.\n\nSee https://github.com/pydata/pandas-datareader/issues\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImmediateDeprecationError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5f30277c8d6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# User pandas_reader.data.DataReader to load the desired data. As simple as that.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mpanel_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'INPX'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'google'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\pandas_datareader\\data.py\u001b[0m in \u001b[0;36mDataReader\u001b[1;34m(name, data_source, start, end, retry_count, pause, session, access_key)\u001b[0m\n\u001b[0;32m    314\u001b[0m                                  \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m                                  \u001b[0mretry_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretry_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpause\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpause\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                                  session=session).read()\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdata_source\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"iex\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\pandas_datareader\\google\\daily.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, symbols, start, end, retry_count, pause, session, chunksize)\u001b[0m\n\u001b[0;32m     34\u001b[0m     def __init__(self, symbols=None, start=None, end=None, retry_count=3,\n\u001b[0;32m     35\u001b[0m                  pause=0.1, session=None, chunksize=25):\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImmediateDeprecationError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEP_ERROR_MSG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Google finance'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         super(GoogleDailyReader, self).__init__(symbols, start, end,\n\u001b[0;32m     38\u001b[0m                                                 \u001b[0mretry_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpause\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImmediateDeprecationError\u001b[0m: \nGoogle finance has been immediately deprecated due to large breaks in the API without the\nintroduction of a stable replacement. Pull Requests to re-enable these data\nconnectors are welcome.\n\nSee https://github.com/pydata/pandas-datareader/issues\n"
     ]
    }
   ],
   "source": [
    "# Define the instruments to download. We would like to see Apple, Microsoft and the S&P500 index.\n",
    "tickers = ['AAPL', 'MSFT', '^GSPC']\n",
    "\n",
    "# We would like all available data from 01/01/2000 until 12/31/2016.\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2016-12-31'\n",
    "\n",
    "# User pandas_reader.data.DataReader to load the desired data. As simple as that.\n",
    "panel_data = data.DataReader('INPX', 'google', start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
